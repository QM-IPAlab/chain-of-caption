<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="Chain-of-Caption: Training-free Improvement of Multimodal Large Language Model on Referring Expression Comprehension" content="">
  <meta property="og:title" content="Chain-of-Caption: Training-free Improvement of Multimodal Large Language Model on Referring Expression Comprehension"/>
  <meta property="og:description" content=""/>
  <meta property="og:url" content="https://qm-ipalab.github.io/chain-of-caption/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="vision-language models, referring expression comprehension, in-context learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Chain-of-Caption: Training-free Improvement of Multimodal Large Language Model on Referring Expression Comprehension</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3 publication-title">Chain-of-Caption: Training-free Improvement of Multimodal Large Language Model on Referring Expression Comprehension</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xPEdLeoAAAAJ&hl=en" target="_blank">Yik Lung Pang</a>,</span>
                  <span class="author-block">
                    <a href="http://eecs.qmul.ac.uk/~coh/" target="_blank">Changjae Oh</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Queen Mary, University of London</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                    
                    <!-- ArXiv abstract Link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2602.08211" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                    </span>
                    <!-- GitHub Code Link -->
                    <span class="link-block">
                      <a href="https://github.com/QM-IPAlab/chain-of-caption" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                    </span>
                  </div>
                  </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as chain-of-thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.
          </p>
        </div>
      <div>
      <img src="static/images/intro.jpg" alt="" width="70%" class="center-image"/>
    </div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="hero section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Proposed method</h2>
          <div class="level-set has-text-justified">
            <p>
              We propose to tackle the referring expression comprehension task using an MLLM in a 2-stage process. (1) Grounded description is generated for the input image using an MLLM. Each line contains a pair of object description and bounding box coordinates. (2) We use the VQA and captioning capability of the MLLM to refine the predicted bounding box in a process named Chain-of-Caption.
            </p>
          </div>
        </div>
        <div class="columns is-centered">
          <img src="static/images/coc.jpg" alt="Robot setup" width="100%" class="center-image"/>
        </div>
      </div>
  </div>
</section>

<!-- Results -->
<section class="hero section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Results</h2>
          <div class="level-set has-text-justified">
            <p>
              Our proposed method Chain-of-Caption improves the prediction accuracy especially at high IoU thresholds, predicting bounding boxes that fits better to the target. Legend: <span style="display:inline-block; width:16px; height:16px; background-color:blue;"></span> - Groundtruth bounding box, <span style="display:inline-block; width:16px; height:16px; background-color:green;"></span> - Predicted bounding box
            </p>
          </div>
        </div>
        <div class="columns is-centered">
          <img src="static/images/vis1.jpg" alt="Robot setup" width="100%" class="center-image"/>
        </div>
        <div class="columns is-centered">
          <img src="static/images/vis2.jpg" alt="Robot setup" width="100%" class="center-image"/>
        </div>
      </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
